{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, i will demonstrate how to preprocess the input document using below example (assume it was load from document loader), then converting it to vector (using ollama) and store all the important information onto mongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a full-stack software and AI engineer especially interested in building web application experiences, scaling systems up, and producing reliable AI applications. I seek a full-time role to apply my skills, embrace challenges, collaborate with diverse teams, and contribute meaningfully to an organization.\n"
     ]
    }
   ],
   "source": [
    "## Example text input, ASSUME ITS LOAD FROM DOCUMENT LOADER PART\n",
    "text = \"\"\"I am a full-stack software and AI engineer especially interested in building web application experiences, scaling systems up, and producing reliable AI applications. I seek a full-time role to apply my skills, embrace challenges, collaborate with diverse teams, and contribute meaningfully to an organization.\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting chunk_size & chunk overlap, besides there are various kind of text splitter, and the most commonly used and perform nicely is recursivecharactertextsplitter, so this project going to implement with this approach. Of course, we can make it to be chooseable by the user too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the value for chunk_size and overlap will be decided by the user, the advisable value will be in bit value, eg. 2,4,8,16,32,64 ...so on , and value for overlap would be 10-20% of the chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'what is these document about'}, page_content='I am a full-stack software and AI'),\n",
       " Document(metadata={'source': 'what is these document about'}, page_content='AI engineer especially interested'),\n",
       " Document(metadata={'source': 'what is these document about'}, page_content='in building web application'),\n",
       " Document(metadata={'source': 'what is these document about'}, page_content='experiences, scaling systems up,'),\n",
       " Document(metadata={'source': 'what is these document about'}, page_content='and producing reliable AI'),\n",
       " Document(metadata={'source': 'what is these document about'}, page_content='AI applications. I seek a'),\n",
       " Document(metadata={'source': 'what is these document about'}, page_content='a full-time role to apply my'),\n",
       " Document(metadata={'source': 'what is these document about'}, page_content='my skills, embrace challenges,'),\n",
       " Document(metadata={'source': 'what is these document about'}, page_content='collaborate with diverse teams,'),\n",
       " Document(metadata={'source': 'what is these document about'}, page_content='and contribute meaningfully to an'),\n",
       " Document(metadata={'source': 'what is these document about'}, page_content='an organization.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 35\n",
    "chunk_overlap = 35//10\n",
    "\n",
    "chunk_option = {\"chunk_size\":chunk_size,\"chunk_overlap\":chunk_overlap}\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\",\"\\n\",\" \",\"\"],**chunk_option)\n",
    "res = text_splitter.create_documents([text],metadatas=[{\"source\":\"what is these document about\"}])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is to demonstrate some basic operation (bulk insertion) to store data onto mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful inserting data onto mongodb\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "## make sure your mongoclient url is not publicly visible, for this example im running in local machine, so is fine.\n",
    "dbConn = MongoClient(\"mongodb://root:rootpass@192.168.1.7:27017/?authSource=admin\")\n",
    "database = dbConn[\"LLM_PROJECT\"]\n",
    "col = database[\"Vector_DB\"]\n",
    "\n",
    "docs = []\n",
    "for doc in res:\n",
    "    doc_to_insert = {\n",
    "        \"content\": doc.page_content,\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"embedding\": \"\",\n",
    "        \"create_date\": datetime.datetime.now(datetime.timezone.utc),\n",
    "        \"update_date\": datetime.datetime.now(datetime.timezone.utc),\n",
    "    }\n",
    "\n",
    "    docs.append(doc_to_insert)\n",
    "\n",
    "col.insert_many(docs)\n",
    "print(\"Successful inserting data onto mongodb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below codesnippet is to showcase how to apply ollama model running locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from ollama import AsyncClient\n",
    "import re\n",
    "\n",
    "host = \"http://localhost:11434\"\n",
    "model = \"deepseek-r1:1.5b\"\n",
    "message = [{\"role\":\"user\",\"content\":\"Hello there, who am i\"}]\n",
    "\n",
    "client = AsyncClient(host=host)\n",
    "\n",
    "response = await client.chat(\n",
    "    model=model,\n",
    "    messages=message,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "def clean_response(text:str) -> str:\n",
    "    return re.sub(r\"<think>\\n*</think>\", \"\", text).strip()\n",
    "\n",
    "result = \"\"\n",
    "async for chunk in response:\n",
    "    result += chunk[\"message\"][\"content\"]\n",
    "print(clean_response(result))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below codesnipet is for embedding&storing data using Ollama & mongoDB using batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Process Done\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "dbConn = MongoClient(\"mongodb://root:rootpass@192.168.1.7:27017/?authSource=admin\")\n",
    "database = dbConn[\"LLM_PROJECT\"]\n",
    "col = database[\"Vector_DB\"]\n",
    "\n",
    "## batch processing for embedding\n",
    "batch_size = 2\n",
    "model = \"nomic-embed-text\"\n",
    "\n",
    "for i in range(0,len(res),batch_size):\n",
    "    batch = res[i:i+batch_size]\n",
    "    batchList = []\n",
    "    for texts in batch:\n",
    "        batchList.append(texts.page_content)\n",
    "    embedding = ollama.embed(model=model,input=batchList)\n",
    "    # print(embedding[\"embeddings\"])\n",
    "    docs_to_insert = []\n",
    "\n",
    "    for j,embed_text in enumerate(embedding.embeddings):\n",
    "        doc = {\n",
    "            \"content\": batch[j].page_content,\n",
    "            \"metadata\": batch[j].metadata,\n",
    "            \"embedding\":embed_text,\n",
    "            \"create_date\": datetime.datetime.now(datetime.timezone.utc),\n",
    "            \"update_date\": datetime.datetime.now(datetime.timezone.utc),\n",
    "        }\n",
    "        docs_to_insert.append(doc)\n",
    "    \n",
    "    if docs_to_insert:\n",
    "        col.insert_many(docs_to_insert)\n",
    "print(f\"Embedding Process Done\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
