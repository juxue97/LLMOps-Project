{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jarvi\\OneDrive\\Desktop\\tehwei\\github\\LLMOps-Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\jarvi\\OneDrive\\Desktop\\tehwei\\github\\LLMOps-Project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jarvi\\.cache\\huggingface\\hub\\datasets--infinite-dataset-hub--HRChatTopicsDataset. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 100/100 [00:00<00:00, 5555.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "name = \"infinite-dataset-hub/HRChatTopicsDataset\"\n",
    "dataset = load_dataset(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 165.79ba/s]\n"
     ]
    }
   ],
   "source": [
    "dataset[\"train\"].to_csv(\"dataset_training.csv\")\n",
    "# If there are multiple splits (e.g., train, validation, test), save each one\n",
    "if \"validation\" in dataset:\n",
    "    dataset[\"validation\"].to_csv(\"dataset_validation.csv\")\n",
    "if \"test\" in dataset:\n",
    "    dataset[\"test\"].to_csv(\"dataset_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>employee_message</th>\n",
       "      <th>bot_response</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-15T08:30:00Z</td>\n",
       "      <td>What are my health benefits?</td>\n",
       "      <td>Your health benefits include dental and vision...</td>\n",
       "      <td>Benefits Inquiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-15T09:15:00Z</td>\n",
       "      <td>How can I apply for maternity leave?</td>\n",
       "      <td>You can apply for maternity leave by submittin...</td>\n",
       "      <td>Leave Policy Inquiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-01-15T10:05:00Z</td>\n",
       "      <td>What are the company's remote work policies?</td>\n",
       "      <td>The company allows remote work with approval f...</td>\n",
       "      <td>Workplace Policy Inquiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-01-15T11:45:00Z</td>\n",
       "      <td>I'm looking to switch departments. Who should ...</td>\n",
       "      <td>You should schedule a meeting with the departm...</td>\n",
       "      <td>Career Advice Inquiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-01-15T12:30:00Z</td>\n",
       "      <td>Is there any training available for new managers?</td>\n",
       "      <td>Yes, we offer a 'New Manager Leadership Progra...</td>\n",
       "      <td>Training Inquiry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx             timestamp  \\\n",
       "0    0  2023-01-15T08:30:00Z   \n",
       "1    1  2023-01-15T09:15:00Z   \n",
       "2    2  2023-01-15T10:05:00Z   \n",
       "3    3  2023-01-15T11:45:00Z   \n",
       "4    4  2023-01-15T12:30:00Z   \n",
       "\n",
       "                                    employee_message  \\\n",
       "0                       What are my health benefits?   \n",
       "1               How can I apply for maternity leave?   \n",
       "2       What are the company's remote work policies?   \n",
       "3  I'm looking to switch departments. Who should ...   \n",
       "4  Is there any training available for new managers?   \n",
       "\n",
       "                                        bot_response                     label  \n",
       "0  Your health benefits include dental and vision...          Benefits Inquiry  \n",
       "1  You can apply for maternity leave by submittin...      Leave Policy Inquiry  \n",
       "2  The company allows remote work with approval f...  Workplace Policy Inquiry  \n",
       "3  You should schedule a meeting with the departm...     Career Advice Inquiry  \n",
       "4  Yes, we offer a 'New Manager Leadership Progra...          Training Inquiry  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset_training.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "Index(['idx', 'timestamp', 'employee_message', 'bot_response', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are my health benefits?\n",
      "Your health benefits include dental and vision coverage.\n"
     ]
    }
   ],
   "source": [
    "print(df[\"employee_message\"][0])\n",
    "print(df[\"bot_response\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['idx', 'timestamp', 'employee_message', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## user must select the answer (target) to be embed, \n",
    "## and the other columns as metadata to be stored (optional).\n",
    "\n",
    "## example\n",
    "# must\n",
    "answer = \"bot_response\"\n",
    "answerColumn = df[answer].tolist()\n",
    "\n",
    "# optional - lets say multiple is selected\n",
    "metadata = [\"idx\",\"timestamp\",\"employee_message\",\"label\"]\n",
    "if metadata and answer not in metadata:\n",
    "    metadataColumn = df[metadata]\n",
    "    print(metadataColumn.columns)\n",
    "else:\n",
    "    metadataColumn = pd.DataFrame()\n",
    "    print(\"selected embed column cannot set as metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for csv, 1 row = 1 chunk\n",
    "# user can define which column to embed, and which column to be metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import datetime\n",
    "import pymongo\n",
    "\n",
    "model = \"nomic-embed-text\"\n",
    "batch_size = 16\n",
    "\n",
    "databaseName = \"LLM_PROJECT\"\n",
    "col = \"Csv_Vector_DB\"\n",
    "\n",
    "dbConn = pymongo.MongoClient(\"mongodb://root:rootpass@192.168.1.8:27017/?authSource=admin\")\n",
    "database = dbConn[databaseName]\n",
    "col = database[col]\n",
    "\n",
    "for i in range(0,len(answerColumn),batch_size):\n",
    "    batch = answerColumn[i:i+batch_size]\n",
    "    if not metadataColumn.empty:\n",
    "        batch_metadata = metadataColumn.iloc[i:i + batch_size].to_dict(orient=\"records\")\n",
    "    else:\n",
    "        batch_metadata = None\n",
    "\n",
    "    embeddings = ollama.embed(model=model,input=batch)\n",
    "    docs_to_insert = []\n",
    "\n",
    "    for j,embed_doc in enumerate(embeddings.embeddings):\n",
    "        doc = {\n",
    "            \"content\": batch[j],\n",
    "            \"metadata\":batch_metadata[j],\n",
    "            \"embedding\":embed_doc,\n",
    "            \"create_date\":datetime.datetime.now(datetime.timezone.utc),\n",
    "            \"update_date\":datetime.datetime.now(datetime.timezone.utc),\n",
    "        }\n",
    "        docs_to_insert.append(doc)\n",
    "\n",
    "    if docs_to_insert:\n",
    "        col.insert_many(docs_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for txt / word / pdf, recursive/semantic\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pathlib import Path\n",
    "path = \"C://Users//jarvi//OneDrive//Desktop//tehwei//github//LLMOps-Project//research//rag//TehHungWei-CV.pdf\"\n",
    "loader = PyPDFLoader(Path(path))\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-02-25T12:19:10+08:00', 'author': 'User', 'moddate': '2025-02-25T12:19:10+08:00', 'source': 'C:\\\\Users\\\\jarvi\\\\OneDrive\\\\Desktop\\\\tehwei\\\\github\\\\LLMOps-Project\\\\research\\\\TehHungWei-CV.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='Selangor, Malaysia \\nLinkedin - \\nhttps://www.linkedin.com/in/teh-\\nhung-wei-b155b3249/ \\nGitHub - \\nhttps://github.com/juxue97 \\n \\nTeh Hung Wei \\nFull Stack Engineer | AI Engineer \\n(+60) 18-323 0302 \\nhwteh1997@gmail.com \\nPortfolio Website - https://port-folio-git-\\nmain-teh-hung-weis-\\nprojects.vercel.app/homepage \\nI am a full-stack software and AI engineer especially interested in building web application experiences, scaling systems \\nup, and producing reliable AI applications. I seek a full-time role to apply my skills, embrace challenges, collaborate with \\ndiverse teams, and contribute meaningfully to an organization. \\nWork Experience \\nAI Software Engineer                               Anhsin Technology Sdn. Bhd. April 2024 – Jan 2025 \\nAI & Backend team        Kuala Lumpur, Malaysia \\n● Developed and deployed an AI-powered chatbot using RAG architecture, integrating OpenAI, LangChain, and \\nFastAPI, improving response accuracy by 40%. \\n● Designed and optimized the system backend in Python, reducing API response time by 30% through efficient \\ndatabase indexing and caching. \\n● Implemented scalable infrastructure on AWS (EC2, S3, RDS), improving system uptime to 99.9% and reducing \\ncloud costs by 15%. \\n● Managed and optimized NoSQL databases (MongoDB), ensuring seamless data storage and retrieval for chatbot \\nsessions. \\n● Integrated streaming APIs using Server-Sent Events (SSE) for real-time chatbot responses, enhancing user \\nengagement. \\n● Developed frontend components using TypeScript and Next.js, building the homepage and login page with a \\nresponsive design. \\n● Implemented authentication middleware for secure access token and refresh token mechanisms, ensuring \\nseamless user authentication and session management. \\nTechnologies, Skills and Languages \\n● Languages: Python, JavaScript, TypeScript, Go \\n● Frameworks: React, NextJS, NestJS, FastAPI \\n● Databases:  MySQL, Postgres, MongoDB, ChromaDB \\n● Cloud:  AWS, Google Cloud Platform \\n● AI Tools: MLflow, Evidently  \\n● Other:  Redis, Docker, CI/CD \\nEducation and Certifications \\n● M.Sc. Computer Science, University of Teknologi, Malaysia.  2022 - 2023 \\n● B.Sc. Industrial Physics, University of Teknologi, Malaysia.  2017 – 2021 \\nProjects \\n1. Scalable Backend System with Go & gRPC Microservices \\na. Overview: \\nDeveloped a robust full-backend system in Go, architected as microservices communicating via gRPC, \\nthis design enables high scalability and maintainability while ensuring efficient inter-service \\ncommunication.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-02-25T12:19:10+08:00', 'author': 'User', 'moddate': '2025-02-25T12:19:10+08:00', 'source': 'C:\\\\Users\\\\jarvi\\\\OneDrive\\\\Desktop\\\\tehwei\\\\github\\\\LLMOps-Project\\\\research\\\\TehHungWei-CV.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='b. Technologies: \\ni. Languages: Go \\nii. Communication & Documentation: gRPC, Swagger \\niii. Infrastructure: Docker, Consul \\niv. Databases: PostgreSQL, MongoDB, Redis \\nv. Messaging & Monitoring: RabbitMQ, Jaeger \\nc. Impact & Outcome: \\ni. Streamlined inter-service communication, supporting rapid scaling and reliable service \\nintegration. \\nii. Enhanced system observability and accelerated issue resolution with Jaeger tracing. \\niii. Established a robust, self-documented API layer that improved team collaboration and external \\nintegration efforts. \\nd. Challenges Addressed: \\ni. Implementing gRPC connections for seamless data exchange between microservices. \\nii. Configuring service discovery and load balancing with Consul. \\niii. Integrating diverse technologies to maintain a consistent API documentation process while \\nensuring data integrity across multiple databases. \\ne. Link: \\nGitHub Repo Project Link - Fullbackend (https://github.com/juxue97/GolangBackend) \\nGitHub Repo Project Link - Microservices (https://github.com/juxue97/GolangProject) \\n \\n2. Chatbot Backend with Self-Hosted LLM Integration \\na. Overview: \\nEngineered a dynamic chatbot backend in Go that integrates with a self-hosted Large Language Model \\n(LLM) using Ollama. The LLM service is deployed on a separate local machine, and an HTTP connection is \\nestablished between the services through strategic DNS and port forwarding configurations. \\nb. Technologies: \\ni. Languages: Go \\nii. AI integration: Self-hosted LLM via Ollama \\niii. Infrastructure: Docker, MongoDB \\niv. Networking: DNS Configuration, Port Forwarding, HTTP communication \\nc. Impact & Outcome: \\ni. Enabled seamless integration between the chatbot backend and the LLM service, ensuring \\nefficient and reliable communication over HTTP. \\nii. Achieved a modular architecture that decouples the AI model from the backend, allowing \\nindependent scaling and maintenance. \\niii. Improved accessibility and security by configuring DNS and port forwarding to maintain a stable \\nconnection between distributed services. \\nd. Challenges Addressed: \\ni. Establishing secure and stable HTTP connections between services hosted on different local \\nmachines. \\nii. Configuring DNS and port forwarding to overcome between services hosted on different local \\nmachines. \\niii. Integrating a self-hosted LLM model into the production environment, balancing performance \\nwith system reliability. \\ne. Link: \\nGithub Repo Project Link - LocalLLM (https://github.com/juxue97/localLLM) \\n \\n3. End-to-End MLOPs Project \\na. Overview: \\nDeveloped a fully automated MLOps pipeline for an end-to-end machine learning project, integrating \\nbest practices in data science, model development, and deployment. The project covers the entire \\nlifecycle from data exploration, model training, and evaluation to deployment with CI/CD automation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-02-25T12:19:10+08:00', 'author': 'User', 'moddate': '2025-02-25T12:19:10+08:00', 'source': 'C:\\\\Users\\\\jarvi\\\\OneDrive\\\\Desktop\\\\tehwei\\\\github\\\\LLMOps-Project\\\\research\\\\TehHungWei-CV.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='b. Technologies: \\ni.   Languages: Python \\nii.   Machine Learning: Scikit-learn, Evidently, Pandas, Matplotlib \\niii.   Infrastructure: Docker, AWS S3, MongoDB \\niv.   MLOps Tools: Evidently (for data drift detection) \\nv.   Deployment & Automation: FastAPI (REST API), GitHub Actions (CI/CD), AWS ECR, EC2 \\nc. Key Features: \\ni. End-to-End ML Pipeline: Developed a modular, OOP-based pipeline covering: \\n• Data ingestion & validation \\n• Data transformation \\n• Model training & fine-tuning (classification task) \\n• Model evaluation & monitoring \\n• Model deployment(AWS EC2) & cloud storage (AWS S3) \\nii. Automated ML Training & Generalization: \\n• Designed a scalable training pipeline that can handle various ML tasks with minimal manual \\nintervention. \\n• Integrated Evidently for real-time model & data drift detection. \\niii. Production-Ready REST API: \\n• Exposed the ML model via FastAPI to serve predictions in production. \\niv. CI/CD with GitHub Actions & AWS: \\n• Ensured zero downtime by automating deployment with GitHub Actions CI/CD to AWS ECR \\n& EC2. \\nd. Impact & Outcome: \\ni. Improved ML workflow efficiency with automated retraining & deployment. \\nii. Scalable and reusable ML pipeline for future projects. \\niii. Reliable model serving with continuous integration & monitoring. \\ne. Link: \\n  Github Repo Project Link - MLOPs-Project (https://github.com/juxue97/MLOPs-Project)')]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selangor, Malaysia Linkedin - https:www.linkedin.cominteh- hung-wei-b155b3249 GitHub - https:github.comjuxue97 Teh Hung Wei Full Stack Engineer AI Engineer (60) 18-323 0302 hwteh1997gmail.com Portfolio Website - https:port-folio-git- main-teh-hung-weis- projects.vercel.apphomepage I am a full-stack software and AI engineer especially interested in building web application experiences, scaling systems up, and producing reliable AI applications. I seek a full-time role to apply my skills, embrace challenges, collaborate with diverse teams, and contribute meaningfully to an organization. Work Experience AI Software Engineer Anhsin Technology Sdn. Bhd. April 2024 Jan 2025 AI Backend team Kuala Lumpur, Malaysia Developed and deployed an AI-powered chatbot using RAG architecture, integrating OpenAI, LangChain, and FastAPI, improving response accuracy by 40. Designed and optimized the system backend in Python, reducing API response time by 30 through efficient database indexing and caching. Implemented scalable infrastructure on AWS (EC2, S3, RDS), improving system uptime to 99.9 and reducing cloud costs by 15. Managed and optimized NoSQL databases (MongoDB), ensuring seamless data storage and retrieval for chatbot sessions. Integrated streaming APIs using Server-Sent Events (SSE) for real-time chatbot responses, enhancing user engagement. Developed frontend components using TypeScript and Next.js, building the homepage and login page with a responsive design. Implemented authentication middleware for secure access token and refresh token mechanisms, ensuring seamless user authentication and session management. Technologies, Skills and Languages Languages: Python, JavaScript, TypeScript, Go Frameworks: React, NextJS, NestJS, FastAPI Databases: MySQL, Postgres, MongoDB, ChromaDB Cloud: AWS, Google Cloud Platform AI Tools: MLflow, Evidently Other: Redis, Docker, CICD Education and Certifications M.Sc. Computer Science, University of Teknologi, Malaysia. 2022 - 2023 B.Sc. Industrial Physics, University of Teknologi, Malaysia. 2017 2021 Projects 1. Scalable Backend System with Go gRPC Microservices a. Overview: Developed a robust full-backend system in Go, architected as microservices communicating via gRPC, this design enables high scalability and maintainability while ensuring efficient inter-service communication.\n",
      "b. Technologies: i. Languages: Go ii. Communication Documentation: gRPC, Swagger iii. Infrastructure: Docker, Consul iv. Databases: PostgreSQL, MongoDB, Redis v. Messaging Monitoring: RabbitMQ, Jaeger c. Impact Outcome: i. Streamlined inter-service communication, supporting rapid scaling and reliable service integration. ii. Enhanced system observability and accelerated issue resolution with Jaeger tracing. iii. Established a robust, self-documented API layer that improved team collaboration and external integration efforts. d. Challenges Addressed: i. Implementing gRPC connections for seamless data exchange between microservices. ii. Configuring service discovery and load balancing with Consul. iii. Integrating diverse technologies to maintain a consistent API documentation process while ensuring data integrity across multiple databases. e. Link: GitHub Repo Project Link - Fullbackend (https:github.comjuxue97GolangBackend) GitHub Repo Project Link - Microservices (https:github.comjuxue97GolangProject) 2. Chatbot Backend with Self-Hosted LLM Integration a. Overview: Engineered a dynamic chatbot backend in Go that integrates with a self-hosted Large Language Model (LLM) using Ollama. The LLM service is deployed on a separate local machine, and an HTTP connection is established between the services through strategic DNS and port forwarding configurations. b. Technologies: i. Languages: Go ii. AI integration: Self-hosted LLM via Ollama iii. Infrastructure: Docker, MongoDB iv. Networking: DNS Configuration, Port Forwarding, HTTP communication c. Impact Outcome: i. Enabled seamless integration between the chatbot backend and the LLM service, ensuring efficient and reliable communication over HTTP. ii. Achieved a modular architecture that decouples the AI model from the backend, allowing independent scaling and maintenance. iii. Improved accessibility and security by configuring DNS and port forwarding to maintain a stable connection between distributed services. d. Challenges Addressed: i. Establishing secure and stable HTTP connections between services hosted on different local machines. ii. Configuring DNS and port forwarding to overcome between services hosted on different local machines. iii. Integrating a self-hosted LLM model into the production environment, balancing performance with system reliability. e. Link: Github Repo Project Link - LocalLLM (https:github.comjuxue97localLLM) 3. End-to-End MLOPs Project a. Overview: Developed a fully automated MLOps pipeline for an end-to-end machine learning project, integrating best practices in data science, model development, and deployment. The project covers the entire lifecycle from data exploration, model training, and evaluation to deployment with CICD automation.\n",
      "b. Technologies: i. Languages: Python ii. Machine Learning: Scikit-learn, Evidently, Pandas, Matplotlib iii. Infrastructure: Docker, AWS S3, MongoDB iv. MLOps Tools: Evidently (for data drift detection) v. Deployment Automation: FastAPI (REST API), GitHub Actions (CICD), AWS ECR, EC2 c. Key Features: i. End-to-End ML Pipeline: Developed a modular, OOP-based pipeline covering: Data ingestion validation Data transformation Model training fine-tuning (classification task) Model evaluation monitoring Model deployment(AWS EC2) cloud storage (AWS S3) ii. Automated ML Training Generalization: Designed a scalable training pipeline that can handle various ML tasks with minimal manual intervention. Integrated Evidently for real-time model data drift detection. iii. Production-Ready REST API: Exposed the ML model via FastAPI to serve predictions in production. iv. CICD with GitHub Actions AWS: Ensured zero downtime by automating deployment with GitHub Actions CICD to AWS ECR EC2. d. Impact Outcome: i. Improved ML workflow efficiency with automated retraining deployment. ii. Scalable and reusable ML pipeline for future projects. iii. Reliable model serving with continuous integration monitoring. e. Link: Github Repo Project Link - MLOPs-Project (https:github.comjuxue97MLOPs-Project)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\t\\n\\r\\f\\v]+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s,.!?;:()-]', '', text) \n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    return text\n",
    "\n",
    "fullText = \"\\n\".join([clean_text(page.page_content) for page in pages])\n",
    "print(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Selangor, Malaysia Linkedin - https:www.linkedin.cominteh- hung-wei-b155b3249 GitHub - https:github.comjuxue97 Teh Hung Wei Full Stack Engineer AI Engineer (60) 18-323 0302 hwteh1997gmail.com Portfolio Website - https:port-folio-git- main-teh-hung-weis-'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='main-teh-hung-weis- projects.vercel.apphomepage I am a full-stack software and AI engineer especially interested in building web application experiences, scaling systems up, and producing reliable AI applications. I seek a full-time role to apply my'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='role to apply my skills, embrace challenges, collaborate with diverse teams, and contribute meaningfully to an organization. Work Experience AI Software Engineer Anhsin Technology Sdn. Bhd. April 2024 Jan 2025 AI Backend team Kuala Lumpur, Malaysia'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Kuala Lumpur, Malaysia Developed and deployed an AI-powered chatbot using RAG architecture, integrating OpenAI, LangChain, and FastAPI, improving response accuracy by 40. Designed and optimized the system backend in Python, reducing API response time by'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='API response time by 30 through efficient database indexing and caching. Implemented scalable infrastructure on AWS (EC2, S3, RDS), improving system uptime to 99.9 and reducing cloud costs by 15. Managed and optimized NoSQL databases (MongoDB), ensuring'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='(MongoDB), ensuring seamless data storage and retrieval for chatbot sessions. Integrated streaming APIs using Server-Sent Events (SSE) for real-time chatbot responses, enhancing user engagement. Developed frontend components using TypeScript and Next.js,'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='TypeScript and Next.js, building the homepage and login page with a responsive design. Implemented authentication middleware for secure access token and refresh token mechanisms, ensuring seamless user authentication and session management. Technologies,'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Technologies, Skills and Languages Languages: Python, JavaScript, TypeScript, Go Frameworks: React, NextJS, NestJS, FastAPI Databases: MySQL, Postgres, MongoDB, ChromaDB Cloud: AWS, Google Cloud Platform AI Tools: MLflow, Evidently Other: Redis, Docker,'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Other: Redis, Docker, CICD Education and Certifications M.Sc. Computer Science, University of Teknologi, Malaysia. 2022 - 2023 B.Sc. Industrial Physics, University of Teknologi, Malaysia. 2017 2021 Projects 1. Scalable Backend System with Go gRPC'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='System with Go gRPC Microservices a. Overview: Developed a robust full-backend system in Go, architected as microservices communicating via gRPC, this design enables high scalability and maintainability while ensuring efficient inter-service'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='efficient inter-service communication.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='b. Technologies: i. Languages: Go ii. Communication Documentation: gRPC, Swagger iii. Infrastructure: Docker, Consul iv. Databases: PostgreSQL, MongoDB, Redis v. Messaging Monitoring: RabbitMQ, Jaeger c. Impact Outcome: i. Streamlined inter-service'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='inter-service communication, supporting rapid scaling and reliable service integration. ii. Enhanced system observability and accelerated issue resolution with Jaeger tracing. iii. Established a robust, self-documented API layer that improved team'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='layer that improved team collaboration and external integration efforts. d. Challenges Addressed: i. Implementing gRPC connections for seamless data exchange between microservices. ii. Configuring service discovery and load balancing with Consul. iii.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='with Consul. iii. Integrating diverse technologies to maintain a consistent API documentation process while ensuring data integrity across multiple databases. e. Link: GitHub Repo Project Link - Fullbackend (https:github.comjuxue97GolangBackend) GitHub'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='GitHub Repo Project Link - Microservices (https:github.comjuxue97GolangProject) 2. Chatbot Backend with Self-Hosted LLM Integration a. Overview: Engineered a dynamic chatbot backend in Go that integrates with a self-hosted Large Language Model (LLM) using'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Model (LLM) using Ollama. The LLM service is deployed on a separate local machine, and an HTTP connection is established between the services through strategic DNS and port forwarding configurations. b. Technologies: i. Languages: Go ii. AI integration:'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Go ii. AI integration: Self-hosted LLM via Ollama iii. Infrastructure: Docker, MongoDB iv. Networking: DNS Configuration, Port Forwarding, HTTP communication c. Impact Outcome: i. Enabled seamless integration between the chatbot backend and the LLM'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='backend and the LLM service, ensuring efficient and reliable communication over HTTP. ii. Achieved a modular architecture that decouples the AI model from the backend, allowing independent scaling and maintenance. iii. Improved accessibility and security'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='and security by configuring DNS and port forwarding to maintain a stable connection between distributed services. d. Challenges Addressed: i. Establishing secure and stable HTTP connections between services hosted on different local machines. ii.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='local machines. ii. Configuring DNS and port forwarding to overcome between services hosted on different local machines. iii. Integrating a self-hosted LLM model into the production environment, balancing performance with system reliability. e. Link:'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='reliability. e. Link: Github Repo Project Link - LocalLLM (https:github.comjuxue97localLLM) 3. End-to-End MLOPs Project a. Overview: Developed a fully automated MLOps pipeline for an end-to-end machine learning project, integrating best practices in data'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='best practices in data science, model development, and deployment. The project covers the entire lifecycle from data exploration, model training, and evaluation to deployment with CICD automation.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='b. Technologies: i. Languages: Python ii. Machine Learning: Scikit-learn, Evidently, Pandas, Matplotlib iii. Infrastructure: Docker, AWS S3, MongoDB iv. MLOps Tools: Evidently (for data drift detection) v. Deployment Automation: FastAPI (REST API), GitHub'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='(REST API), GitHub Actions (CICD), AWS ECR, EC2 c. Key Features: i. End-to-End ML Pipeline: Developed a modular, OOP-based pipeline covering: Data ingestion validation Data transformation Model training fine-tuning (classification task) Model evaluation'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='task) Model evaluation monitoring Model deployment(AWS EC2) cloud storage (AWS S3) ii. Automated ML Training Generalization: Designed a scalable training pipeline that can handle various ML tasks with minimal manual intervention. Integrated Evidently for'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Integrated Evidently for real-time model data drift detection. iii. Production-Ready REST API: Exposed the ML model via FastAPI to serve predictions in production. iv. CICD with GitHub Actions AWS: Ensured zero downtime by automating deployment with'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='deployment with GitHub Actions CICD to AWS ECR EC2. d. Impact Outcome: i. Improved ML workflow efficiency with automated retraining deployment. ii. Scalable and reusable ML pipeline for future projects. iii. Reliable model serving with continuous'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='serving with continuous integration monitoring. e. Link: Github Repo Project Link - MLOPs-Project (https:github.comjuxue97MLOPs-Project)')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 256\n",
    "chunk_overlap = chunk_size//10\n",
    "file_name = \"ResumeCVTemplate\"\n",
    "\n",
    "chunk_option = {\"chunk_size\":chunk_size,\"chunk_overlap\":chunk_overlap}\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\",\"\\n\",\" \",\"\"],**chunk_option)\n",
    "res = text_splitter.create_documents([fullText],metadatas=[{\"source\":\"ResumeCVTemplate\"}])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\jarvi\\\\OneDrive\\\\Desktop\\\\tehwei\\\\github\\\\LLMOps-Project\\\\research\\\\TehHungWei-CVTemplate - 1 -.docx'}, page_content='Selangor, Malaysia\\nLinkedin - https://www.linkedin.com/in/teh-hung-wei-b155b3249/\\n\\n\\t\\t\\tGitHub - https://github.com/juxue97\\n\\n\\t\\t\\t\\n\\n\\t\\t\\tTeh Hung Wei\\n\\nFull Stack Engineer | AI Engineer\\n\\n\\t\\t\\t(+60) 18-323 0302 hwteh1997@gmail.com\\n\\n\\t\\t\\tPortfolio Website - https://port-folio-git-main-teh-hung-weis-projects.vercel.app/homepage\\n\\nI am a full-stack software and AI engineer especially interested in building web application experiences, scaling systems up, and producing reliable AI applications. I seek a full-time role to apply my skills, embrace challenges, collaborate with diverse teams, and contribute meaningfully to an organization.\\n\\nWork Experience\\n\\n\\t\\t\\t\\t\\t\\tAI Software Engineer                               Anhsin Technology Sdn. Bhd.\\tApril 2024 – Jan 2025\\n\\n\\t\\t\\t\\tAI & Backend team\\t       Kuala Lumpur, Malaysia\\n\\n\\t\\t\\tDeveloped and deployed an AI-powered chatbot using RAG architecture, integrating OpenAI, LangChain, and FastAPI, improving response accuracy by 40%.\\n\\n\\t\\t\\tDesigned and optimized the system backend in Python, reducing API response time by 30% through efficient database indexing and caching.\\n\\n\\t\\t\\tImplemented scalable infrastructure on AWS (EC2, S3, RDS), improving system uptime to 99.9% and reducing cloud costs by 15%.\\n\\n\\t\\t\\tManaged and optimized NoSQL databases (MongoDB), ensuring seamless data storage and retrieval for chatbot sessions.\\n\\n\\t\\t\\tIntegrated streaming APIs using Server-Sent Events (SSE) for real-time chatbot responses, enhancing user engagement.\\n\\n\\t\\t\\tDeveloped frontend components using TypeScript and Next.js, building the homepage and login page with a responsive design.\\n\\n\\t\\t\\t\\tImplemented authentication middleware for secure access token and refresh token mechanisms, ensuring seamless user authentication and session management.\\n\\nTechnologies, Skills and Languages\\n\\nLanguages:\\tPython, JavaScript, TypeScript, Go\\n\\nFrameworks:\\tReact, NextJS, NestJS, FastAPI\\n\\nDatabases: \\tMySQL, Postgres, MongoDB, ChromaDB\\n\\nCloud:\\t\\tAWS, Google Cloud Platform\\n\\nAI Tools:\\tMLflow, Evidently\\t\\n\\nOther:\\t\\tRedis, Docker, CI/CD\\n\\nEducation and Certifications\\n\\n\\t\\t\\t\\tM.Sc. Computer Science, University of Teknologi, Malaysia. \\t2022 - 2023\\n\\n\\tB.Sc. Industrial Physics, University of Teknologi, Malaysia. \\t2017 – 2021\\n\\nProjects\\n\\nScalable Backend System with Go & gRPC Microservices\\n\\nOverview:\\n\\nDeveloped a robust full-backend system in Go, architected as microservices communicating via gRPC, this design enables high scalability and maintainability while ensuring efficient inter-service communication.\\n\\nTechnologies:\\n\\nLanguages: Go\\n\\nCommunication & Documentation: gRPC, Swagger\\n\\nInfrastructure: Docker, Consul\\n\\nDatabases: PostgreSQL, MongoDB, Redis\\n\\nMessaging & Monitoring: RabbitMQ, Jaeger\\n\\nImpact & Outcome:\\n\\nStreamlined inter-service communication, supporting rapid scaling and reliable service integration.\\n\\nEnhanced system observability and accelerated issue resolution with Jaeger tracing.\\n\\nEstablished a robust, self-documented API layer that improved team collaboration and external integration efforts.\\n\\nChallenges Addressed:\\n\\nImplementing gRPC connections for seamless data exchange between microservices.\\n\\nConfiguring service discovery and load balancing with Consul.\\n\\nIntegrating diverse technologies to maintain a consistent API documentation process while ensuring data integrity across multiple databases.\\n\\nLink:\\n\\nGitHub Repo Project Link - Fullbackend (https://github.com/juxue97/GolangBackend)\\n\\nGitHub Repo Project Link - Microservices (https://github.com/juxue97/GolangProject)\\n\\n\\n\\nChatbot Backend with Self-Hosted LLM Integration\\n\\nOverview:\\n\\nEngineered a dynamic chatbot backend in Go that integrates with a self-hosted Large Language Model (LLM) using Ollama. The LLM service is deployed on a separate local machine, and an HTTP connection is established between the services through strategic DNS and port forwarding configurations.\\n\\nTechnologies:\\n\\nLanguages: Go\\n\\nAI integration: Self-hosted LLM via Ollama\\n\\nInfrastructure: Docker, MongoDB\\n\\nNetworking: DNS Configuration, Port Forwarding, HTTP communication\\n\\nImpact & Outcome:\\n\\nEnabled seamless integration between the chatbot backend and the LLM service, ensuring efficient and reliable communication over HTTP.\\n\\nAchieved a modular architecture that decouples the AI model from the backend, allowing independent scaling and maintenance.\\n\\nImproved accessibility and security by configuring DNS and port forwarding to maintain a stable connection between distributed services.\\n\\nChallenges Addressed:\\n\\nEstablishing secure and stable HTTP connections between services hosted on different local machines.\\n\\nConfiguring DNS and port forwarding to overcome between services hosted on different local machines.\\n\\n\\t\\t\\tIntegrating a self-hosted LLM model into the production environment, balancing performance with system reliability.\\n\\n\\t\\t\\tLink:\\n\\n\\t\\t\\tGithub Repo Project Link - LocalLLM (https://github.com/juxue97/localLLM)\\n\\n\\t\\t\\t\\n\\nEnd-to-End MLOPs Project\\n\\nOverview:\\n\\nDeveloped a fully automated MLOps pipeline for an end-to-end machine learning project, integrating best practices in data science, model development, and deployment. The project covers the entire lifecycle from data exploration, model training, and evaluation to deployment with CI/CD automation.\\n\\nTechnologies:\\n\\n  Languages: Python\\n\\n  Machine Learning: Scikit-learn, Evidently, Pandas, Matplotlib\\n\\n  Infrastructure: Docker, AWS S3, MongoDB\\n\\n  MLOps Tools: Evidently (for data drift detection)\\n\\n  Deployment & Automation: FastAPI (REST API), GitHub Actions (CI/CD), AWS ECR, EC2\\n\\nKey Features:\\n\\nEnd-to-End ML Pipeline: Developed a modular, OOP-based pipeline covering:\\n\\nData ingestion & validation\\n\\nData transformation\\n\\nModel training & fine-tuning (classification task)\\n\\nModel evaluation & monitoring\\n\\nModel deployment(AWS EC2) & cloud storage (AWS S3)\\n\\nAutomated ML Training & Generalization:\\n\\n\\t\\tDesigned a scalable training pipeline that can handle various ML tasks with minimal manual intervention.\\n\\n\\t\\tIntegrated Evidently for real-time model & data drift detection.\\n\\nProduction-Ready REST API:\\n\\nExposed the ML model via FastAPI to serve predictions in production.\\n\\nCI/CD with GitHub Actions & AWS:\\n\\nEnsured zero downtime by automating deployment with GitHub Actions CI/CD to AWS ECR & EC2.\\n\\nImpact & Outcome:\\n\\nImproved ML workflow efficiency with automated retraining & deployment.\\n\\nScalable and reusable ML pipeline for future projects.\\n\\n\\t\\t\\tReliable model serving with continuous integration & monitoring.\\n\\n\\t\\t\\tLink:\\n\\n\\t\\t\\t\\t\\tGithub Repo Project Link - MLOPs-Project (https://github.com/juxue97/MLOPs-Project)')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from pathlib import Path\n",
    "\n",
    "path = \"C://Users//jarvi//OneDrive//Desktop//tehwei//github//LLMOps-Project//research//rag//TehHungWei-CVTemplate - 1 -.docx\"\n",
    "loader = Docx2txtLoader(Path(path))\n",
    "docs = loader.load()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Selangor, Malaysia Linkedin - https:www.linkedin.cominteh-hung-wei-b155b3249 GitHub - https:github.comjuxue97 Teh Hung Wei Full Stack Engineer AI Engineer (60) 18-323 0302 hwteh1997gmail.com Portfolio Website - https:port-folio-git-main-teh-hung-weis-projects.vercel.apphomepage I am a full-stack software and AI engineer especially interested in building web application experiences, scaling systems up, and producing reliable AI applications. I seek a full-time role to apply my skills, embrace challenges, collaborate with diverse teams, and contribute meaningfully to an organization. Work Experience AI Software Engineer Anhsin Technology Sdn. Bhd. April 2024 Jan 2025 AI Backend team Kuala Lumpur, Malaysia Developed and deployed an AI-powered chatbot using RAG architecture, integrating OpenAI, LangChain, and FastAPI, improving response accuracy by 40. Designed and optimized the system backend in Python, reducing API response time by 30 through efficient database indexing and caching. Implemented scalable infrastructure on AWS (EC2, S3, RDS), improving system uptime to 99.9 and reducing cloud costs by 15. Managed and optimized NoSQL databases (MongoDB), ensuring seamless data storage and retrieval for chatbot sessions. Integrated streaming APIs using Server-Sent Events (SSE) for real-time chatbot responses, enhancing user engagement. Developed frontend components using TypeScript and Next.js, building the homepage and login page with a responsive design. Implemented authentication middleware for secure access token and refresh token mechanisms, ensuring seamless user authentication and session management. Technologies, Skills and Languages Languages: Python, JavaScript, TypeScript, Go Frameworks: React, NextJS, NestJS, FastAPI Databases: MySQL, Postgres, MongoDB, ChromaDB Cloud: AWS, Google Cloud Platform AI Tools: MLflow, Evidently Other: Redis, Docker, CICD Education and Certifications M.Sc. Computer Science, University of Teknologi, Malaysia. 2022 - 2023 B.Sc. Industrial Physics, University of Teknologi, Malaysia. 2017 2021 Projects Scalable Backend System with Go gRPC Microservices Overview: Developed a robust full-backend system in Go, architected as microservices communicating via gRPC, this design enables high scalability and maintainability while ensuring efficient inter-service communication. Technologies: Languages: Go Communication Documentation: gRPC, Swagger Infrastructure: Docker, Consul Databases: PostgreSQL, MongoDB, Redis Messaging Monitoring: RabbitMQ, Jaeger Impact Outcome: Streamlined inter-service communication, supporting rapid scaling and reliable service integration. Enhanced system observability and accelerated issue resolution with Jaeger tracing. Established a robust, self-documented API layer that improved team collaboration and external integration efforts. Challenges Addressed: Implementing gRPC connections for seamless data exchange between microservices. Configuring service discovery and load balancing with Consul. Integrating diverse technologies to maintain a consistent API documentation process while ensuring data integrity across multiple databases. Link: GitHub Repo Project Link - Fullbackend (https:github.comjuxue97GolangBackend) GitHub Repo Project Link - Microservices (https:github.comjuxue97GolangProject) Chatbot Backend with Self-Hosted LLM Integration Overview: Engineered a dynamic chatbot backend in Go that integrates with a self-hosted Large Language Model (LLM) using Ollama. The LLM service is deployed on a separate local machine, and an HTTP connection is established between the services through strategic DNS and port forwarding configurations. Technologies: Languages: Go AI integration: Self-hosted LLM via Ollama Infrastructure: Docker, MongoDB Networking: DNS Configuration, Port Forwarding, HTTP communication Impact Outcome: Enabled seamless integration between the chatbot backend and the LLM service, ensuring efficient and reliable communication over HTTP. Achieved a modular architecture that decouples the AI model from the backend, allowing independent scaling and maintenance. Improved accessibility and security by configuring DNS and port forwarding to maintain a stable connection between distributed services. Challenges Addressed: Establishing secure and stable HTTP connections between services hosted on different local machines. Configuring DNS and port forwarding to overcome between services hosted on different local machines. Integrating a self-hosted LLM model into the production environment, balancing performance with system reliability. Link: Github Repo Project Link - LocalLLM (https:github.comjuxue97localLLM) End-to-End MLOPs Project Overview: Developed a fully automated MLOps pipeline for an end-to-end machine learning project, integrating best practices in data science, model development, and deployment. The project covers the entire lifecycle from data exploration, model training, and evaluation to deployment with CICD automation. Technologies: Languages: Python Machine Learning: Scikit-learn, Evidently, Pandas, Matplotlib Infrastructure: Docker, AWS S3, MongoDB MLOps Tools: Evidently (for data drift detection) Deployment Automation: FastAPI (REST API), GitHub Actions (CICD), AWS ECR, EC2 Key Features: End-to-End ML Pipeline: Developed a modular, OOP-based pipeline covering: Data ingestion validation Data transformation Model training fine-tuning (classification task) Model evaluation monitoring Model deployment(AWS EC2) cloud storage (AWS S3) Automated ML Training Generalization: Designed a scalable training pipeline that can handle various ML tasks with minimal manual intervention. Integrated Evidently for real-time model data drift detection. Production-Ready REST API: Exposed the ML model via FastAPI to serve predictions in production. CICD with GitHub Actions AWS: Ensured zero downtime by automating deployment with GitHub Actions CICD to AWS ECR EC2. Impact Outcome: Improved ML workflow efficiency with automated retraining deployment. Scalable and reusable ML pipeline for future projects. Reliable model serving with continuous integration monitoring. Link: Github Repo Project Link - MLOPs-Project (https:github.comjuxue97MLOPs-Project)'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes unwanted characters and normalizes spacing.\"\"\"\n",
    "    text = re.sub(r'[\\t\\n\\r\\f\\v]+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s,.!?;:()-]', '', text) \n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    return text\n",
    "\n",
    "full_text = \" \".join([doc.page_content for doc in docs])\n",
    "full_text = clean_text(full_text)  # Apply the cleaning function\n",
    "\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Selangor, Malaysia Linkedin - https:www.linkedin.cominteh- hung-wei-b155b3249 GitHub - https:github.comjuxue97 Teh Hung Wei Full Stack Engineer AI Engineer (60) 18-323 0302 hwteh1997gmail.com Portfolio Website - https:port-folio-git- main-teh-hung-weis-'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='main-teh-hung-weis- projects.vercel.apphomepage I am a full-stack software and AI engineer especially interested in building web application experiences, scaling systems up, and producing reliable AI applications. I seek a full-time role to apply my'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='role to apply my skills, embrace challenges, collaborate with diverse teams, and contribute meaningfully to an organization. Work Experience AI Software Engineer Anhsin Technology Sdn. Bhd. April 2024 Jan 2025 AI Backend team Kuala Lumpur, Malaysia'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Kuala Lumpur, Malaysia Developed and deployed an AI-powered chatbot using RAG architecture, integrating OpenAI, LangChain, and FastAPI, improving response accuracy by 40. Designed and optimized the system backend in Python, reducing API response time by'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='API response time by 30 through efficient database indexing and caching. Implemented scalable infrastructure on AWS (EC2, S3, RDS), improving system uptime to 99.9 and reducing cloud costs by 15. Managed and optimized NoSQL databases (MongoDB), ensuring'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='(MongoDB), ensuring seamless data storage and retrieval for chatbot sessions. Integrated streaming APIs using Server-Sent Events (SSE) for real-time chatbot responses, enhancing user engagement. Developed frontend components using TypeScript and Next.js,'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='TypeScript and Next.js, building the homepage and login page with a responsive design. Implemented authentication middleware for secure access token and refresh token mechanisms, ensuring seamless user authentication and session management. Technologies,'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Technologies, Skills and Languages Languages: Python, JavaScript, TypeScript, Go Frameworks: React, NextJS, NestJS, FastAPI Databases: MySQL, Postgres, MongoDB, ChromaDB Cloud: AWS, Google Cloud Platform AI Tools: MLflow, Evidently Other: Redis, Docker,'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Other: Redis, Docker, CICD Education and Certifications M.Sc. Computer Science, University of Teknologi, Malaysia. 2022 - 2023 B.Sc. Industrial Physics, University of Teknologi, Malaysia. 2017 2021 Projects 1. Scalable Backend System with Go gRPC'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='System with Go gRPC Microservices a. Overview: Developed a robust full-backend system in Go, architected as microservices communicating via gRPC, this design enables high scalability and maintainability while ensuring efficient inter-service'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='efficient inter-service communication.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='b. Technologies: i. Languages: Go ii. Communication Documentation: gRPC, Swagger iii. Infrastructure: Docker, Consul iv. Databases: PostgreSQL, MongoDB, Redis v. Messaging Monitoring: RabbitMQ, Jaeger c. Impact Outcome: i. Streamlined inter-service'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='inter-service communication, supporting rapid scaling and reliable service integration. ii. Enhanced system observability and accelerated issue resolution with Jaeger tracing. iii. Established a robust, self-documented API layer that improved team'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='layer that improved team collaboration and external integration efforts. d. Challenges Addressed: i. Implementing gRPC connections for seamless data exchange between microservices. ii. Configuring service discovery and load balancing with Consul. iii.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='with Consul. iii. Integrating diverse technologies to maintain a consistent API documentation process while ensuring data integrity across multiple databases. e. Link: GitHub Repo Project Link - Fullbackend (https:github.comjuxue97GolangBackend) GitHub'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='GitHub Repo Project Link - Microservices (https:github.comjuxue97GolangProject) 2. Chatbot Backend with Self-Hosted LLM Integration a. Overview: Engineered a dynamic chatbot backend in Go that integrates with a self-hosted Large Language Model (LLM) using'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Model (LLM) using Ollama. The LLM service is deployed on a separate local machine, and an HTTP connection is established between the services through strategic DNS and port forwarding configurations. b. Technologies: i. Languages: Go ii. AI integration:'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Go ii. AI integration: Self-hosted LLM via Ollama iii. Infrastructure: Docker, MongoDB iv. Networking: DNS Configuration, Port Forwarding, HTTP communication c. Impact Outcome: i. Enabled seamless integration between the chatbot backend and the LLM'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='backend and the LLM service, ensuring efficient and reliable communication over HTTP. ii. Achieved a modular architecture that decouples the AI model from the backend, allowing independent scaling and maintenance. iii. Improved accessibility and security'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='and security by configuring DNS and port forwarding to maintain a stable connection between distributed services. d. Challenges Addressed: i. Establishing secure and stable HTTP connections between services hosted on different local machines. ii.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='local machines. ii. Configuring DNS and port forwarding to overcome between services hosted on different local machines. iii. Integrating a self-hosted LLM model into the production environment, balancing performance with system reliability. e. Link:'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='reliability. e. Link: Github Repo Project Link - LocalLLM (https:github.comjuxue97localLLM) 3. End-to-End MLOPs Project a. Overview: Developed a fully automated MLOps pipeline for an end-to-end machine learning project, integrating best practices in data'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='best practices in data science, model development, and deployment. The project covers the entire lifecycle from data exploration, model training, and evaluation to deployment with CICD automation.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='b. Technologies: i. Languages: Python ii. Machine Learning: Scikit-learn, Evidently, Pandas, Matplotlib iii. Infrastructure: Docker, AWS S3, MongoDB iv. MLOps Tools: Evidently (for data drift detection) v. Deployment Automation: FastAPI (REST API), GitHub'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='(REST API), GitHub Actions (CICD), AWS ECR, EC2 c. Key Features: i. End-to-End ML Pipeline: Developed a modular, OOP-based pipeline covering: Data ingestion validation Data transformation Model training fine-tuning (classification task) Model evaluation'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='task) Model evaluation monitoring Model deployment(AWS EC2) cloud storage (AWS S3) ii. Automated ML Training Generalization: Designed a scalable training pipeline that can handle various ML tasks with minimal manual intervention. Integrated Evidently for'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Integrated Evidently for real-time model data drift detection. iii. Production-Ready REST API: Exposed the ML model via FastAPI to serve predictions in production. iv. CICD with GitHub Actions AWS: Ensured zero downtime by automating deployment with'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='deployment with GitHub Actions CICD to AWS ECR EC2. d. Impact Outcome: i. Improved ML workflow efficiency with automated retraining deployment. ii. Scalable and reusable ML pipeline for future projects. iii. Reliable model serving with continuous'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='serving with continuous integration monitoring. e. Link: Github Repo Project Link - MLOPs-Project (https:github.comjuxue97MLOPs-Project)')]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 256\n",
    "chunk_overlap = chunk_size//10\n",
    "file_name = \"ResumeCVTemplate\"\n",
    "\n",
    "chunk_option = {\"chunk_size\":chunk_size,\"chunk_overlap\":chunk_overlap}\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\",\"\\n\",\" \",\"\"],**chunk_option)\n",
    "res = text_splitter.create_documents([fullText],metadatas=[{\"source\":file_name}])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extra: Remove \\n from the chunk\n",
    "for i in range(len(res)):\n",
    "    res[i].page_content = res[i].page_content.replace(\"\\t\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Selangor, Malaysia Linkedin - https:www.linkedin.cominteh- hung-wei-b155b3249 GitHub - https:github.comjuxue97 Teh Hung Wei Full Stack Engineer AI Engineer (60) 18-323 0302 hwteh1997gmail.com Portfolio Website - https:port-folio-git- main-teh-hung-weis-'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='main-teh-hung-weis- projects.vercel.apphomepage I am a full-stack software and AI engineer especially interested in building web application experiences, scaling systems up, and producing reliable AI applications. I seek a full-time role to apply my'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='role to apply my skills, embrace challenges, collaborate with diverse teams, and contribute meaningfully to an organization. Work Experience AI Software Engineer Anhsin Technology Sdn. Bhd. April 2024 Jan 2025 AI Backend team Kuala Lumpur, Malaysia'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Kuala Lumpur, Malaysia Developed and deployed an AI-powered chatbot using RAG architecture, integrating OpenAI, LangChain, and FastAPI, improving response accuracy by 40. Designed and optimized the system backend in Python, reducing API response time by'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='API response time by 30 through efficient database indexing and caching. Implemented scalable infrastructure on AWS (EC2, S3, RDS), improving system uptime to 99.9 and reducing cloud costs by 15. Managed and optimized NoSQL databases (MongoDB), ensuring'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='(MongoDB), ensuring seamless data storage and retrieval for chatbot sessions. Integrated streaming APIs using Server-Sent Events (SSE) for real-time chatbot responses, enhancing user engagement. Developed frontend components using TypeScript and Next.js,'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='TypeScript and Next.js, building the homepage and login page with a responsive design. Implemented authentication middleware for secure access token and refresh token mechanisms, ensuring seamless user authentication and session management. Technologies,'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Technologies, Skills and Languages Languages: Python, JavaScript, TypeScript, Go Frameworks: React, NextJS, NestJS, FastAPI Databases: MySQL, Postgres, MongoDB, ChromaDB Cloud: AWS, Google Cloud Platform AI Tools: MLflow, Evidently Other: Redis, Docker,'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Other: Redis, Docker, CICD Education and Certifications M.Sc. Computer Science, University of Teknologi, Malaysia. 2022 - 2023 B.Sc. Industrial Physics, University of Teknologi, Malaysia. 2017 2021 Projects 1. Scalable Backend System with Go gRPC'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='System with Go gRPC Microservices a. Overview: Developed a robust full-backend system in Go, architected as microservices communicating via gRPC, this design enables high scalability and maintainability while ensuring efficient inter-service'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='efficient inter-service communication.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='b. Technologies: i. Languages: Go ii. Communication Documentation: gRPC, Swagger iii. Infrastructure: Docker, Consul iv. Databases: PostgreSQL, MongoDB, Redis v. Messaging Monitoring: RabbitMQ, Jaeger c. Impact Outcome: i. Streamlined inter-service'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='inter-service communication, supporting rapid scaling and reliable service integration. ii. Enhanced system observability and accelerated issue resolution with Jaeger tracing. iii. Established a robust, self-documented API layer that improved team'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='layer that improved team collaboration and external integration efforts. d. Challenges Addressed: i. Implementing gRPC connections for seamless data exchange between microservices. ii. Configuring service discovery and load balancing with Consul. iii.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='with Consul. iii. Integrating diverse technologies to maintain a consistent API documentation process while ensuring data integrity across multiple databases. e. Link: GitHub Repo Project Link - Fullbackend (https:github.comjuxue97GolangBackend) GitHub'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='GitHub Repo Project Link - Microservices (https:github.comjuxue97GolangProject) 2. Chatbot Backend with Self-Hosted LLM Integration a. Overview: Engineered a dynamic chatbot backend in Go that integrates with a self-hosted Large Language Model (LLM) using'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Model (LLM) using Ollama. The LLM service is deployed on a separate local machine, and an HTTP connection is established between the services through strategic DNS and port forwarding configurations. b. Technologies: i. Languages: Go ii. AI integration:'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Go ii. AI integration: Self-hosted LLM via Ollama iii. Infrastructure: Docker, MongoDB iv. Networking: DNS Configuration, Port Forwarding, HTTP communication c. Impact Outcome: i. Enabled seamless integration between the chatbot backend and the LLM'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='backend and the LLM service, ensuring efficient and reliable communication over HTTP. ii. Achieved a modular architecture that decouples the AI model from the backend, allowing independent scaling and maintenance. iii. Improved accessibility and security'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='and security by configuring DNS and port forwarding to maintain a stable connection between distributed services. d. Challenges Addressed: i. Establishing secure and stable HTTP connections between services hosted on different local machines. ii.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='local machines. ii. Configuring DNS and port forwarding to overcome between services hosted on different local machines. iii. Integrating a self-hosted LLM model into the production environment, balancing performance with system reliability. e. Link:'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='reliability. e. Link: Github Repo Project Link - LocalLLM (https:github.comjuxue97localLLM) 3. End-to-End MLOPs Project a. Overview: Developed a fully automated MLOps pipeline for an end-to-end machine learning project, integrating best practices in data'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='best practices in data science, model development, and deployment. The project covers the entire lifecycle from data exploration, model training, and evaluation to deployment with CICD automation.'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='b. Technologies: i. Languages: Python ii. Machine Learning: Scikit-learn, Evidently, Pandas, Matplotlib iii. Infrastructure: Docker, AWS S3, MongoDB iv. MLOps Tools: Evidently (for data drift detection) v. Deployment Automation: FastAPI (REST API), GitHub'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='(REST API), GitHub Actions (CICD), AWS ECR, EC2 c. Key Features: i. End-to-End ML Pipeline: Developed a modular, OOP-based pipeline covering: Data ingestion validation Data transformation Model training fine-tuning (classification task) Model evaluation'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='task) Model evaluation monitoring Model deployment(AWS EC2) cloud storage (AWS S3) ii. Automated ML Training Generalization: Designed a scalable training pipeline that can handle various ML tasks with minimal manual intervention. Integrated Evidently for'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='Integrated Evidently for real-time model data drift detection. iii. Production-Ready REST API: Exposed the ML model via FastAPI to serve predictions in production. iv. CICD with GitHub Actions AWS: Ensured zero downtime by automating deployment with'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='deployment with GitHub Actions CICD to AWS ECR EC2. d. Impact Outcome: i. Improved ML workflow efficiency with automated retraining deployment. ii. Scalable and reusable ML pipeline for future projects. iii. Reliable model serving with continuous'),\n",
       " Document(metadata={'source': 'ResumeCVTemplate'}, page_content='serving with continuous integration monitoring. e. Link: Github Repo Project Link - MLOPs-Project (https:github.comjuxue97MLOPs-Project)')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\jarvi\\\\OneDrive\\\\Desktop\\\\tehwei\\\\github\\\\LLMOps-Project\\\\research\\\\self_introduction.txt'}, page_content=\"Hello, my name is Teh Hung Wei, and I recently completed my Master's in Computer Science with a focus on data science. \\n\\nThroughout my academic journey, I've been passionate about exploring new knowledge and skills to enhance my value in the field. \\nDuring my studies, I delved deeply into various areas of data science, including machine learning, deep learning, natural language processing, and computer vision. \\nI've had the opportunity to apply these skills in practical settings through projects \\nsuch as sentiment analysis, some classification and regression task or multimodal biometric authentication problem. \\nThese experiences have not only honed my technical abilities but also instilled in me a strong sense of problem-solving and adaptability.\\n\\nOne of the most rewarding aspects of my journey has been the opportunity to see the direct impact of my work. \\nFor instance, the Maple Bot project allowed me to sharpen my programming skills, particularly in handling the performance of my program, such as program's space and time complexity. \\nAdditionally, the deployment of the custom-trained object detection model was satisfying as I was able to put into practice what I learned from theory.\\n\\nRecently, I've been dedicated to further extending my knowldege, particularly in the realm of big data. \\nI've been actively exploring frameworks like PySpark in Databricks, aiming to apply big data tools to handle large-scale projects effectively.\\n\\nSpeaking of my own personality, i am also an optimistic person, where my life motto would be 'happy go lucky', as long as we tried hard or did our best, \\nDon overlook the result, make it better next time.\\nAdditionally, i liked to work with different group of peoples as i believe best ideas come from discussion. \\nI am eager to continue learning and growing in this dynamic field.\\n\\nSo, i am excited to continue my journey with FootfallCam and make some contribution to the company with my skills and knowledge. \\n\\n\\n1. introduce name, study background\\n2. relevant fields, example\\n3. rewarding projects\\n4. personality\\n5. thankyou\")]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from pathlib import Path\n",
    "\n",
    "path = \"C://Users//jarvi//OneDrive//Desktop//tehwei//github//LLMOps-Project//research//rag//self_introduction.txt\"\n",
    "\n",
    "loader = TextLoader(Path(path),encoding=\"utf-8\")\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my name is Teh Hung Wei, and I recently completed my Masters in Computer Science with a focus on data science. Throughout my academic journey, Ive been passionate about exploring new knowledge and skills to enhance my value in the field. During my studies, I delved deeply into various areas of data science, including machine learning, deep learning, natural language processing, and computer vision. Ive had the opportunity to apply these skills in practical settings through projects such as sentiment analysis, some classification and regression task or multimodal biometric authentication problem. These experiences have not only honed my technical abilities but also instilled in me a strong sense of problem-solving and adaptability. One of the most rewarding aspects of my journey has been the opportunity to see the direct impact of my work. For instance, the Maple Bot project allowed me to sharpen my programming skills, particularly in handling the performance of my program, such as programs space and time complexity. Additionally, the deployment of the custom-trained object detection model was satisfying as I was able to put into practice what I learned from theory. Recently, Ive been dedicated to further extending my knowldege, particularly in the realm of big data. Ive been actively exploring frameworks like PySpark in Databricks, aiming to apply big data tools to handle large-scale projects effectively. Speaking of my own personality, i am also an optimistic person, where my life motto would be happy go lucky, as long as we tried hard or did our best, Don overlook the result, make it better next time. Additionally, i liked to work with different group of peoples as i believe best ideas come from discussion. I am eager to continue learning and growing in this dynamic field. So, i am excited to continue my journey with FootfallCam and make some contribution to the company with my skills and knowledge. 1. introduce name, study background 2. relevant fields, example 3. rewarding projects 4. personality 5. thankyou'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = clean_text(doc[0].page_content)\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\jarvi\\\\OneDrive\\\\Desktop\\\\tehwei\\\\github\\\\LLMOps-Project\\\\research\\\\Exposing Python Metrics with Prometheus _ by Adso _ Medium _ Medium.html', 'title': 'Exposing Python Metrics with Prometheus | by Adso | Medium | Medium'}, page_content='Exposing Python Metrics with Prometheus | by Adso | Medium | MediumOpen in appSign upSign inWriteSign upSign inExposing Python Metrics with PrometheusAdso·Follow4 min read·Jun 26, 2023--ListenSharePrometheus is a powerful monitoring and alerting system that collects and stores time-series data. In this step-by-step guide, we will demonstrate how to expose metrics for a simple Python API app and monitor them using Prometheus. We will also instrument the app to track request metrics. To make it easier for you to try it out, we will provide all the necessary code blocks and a Docker Compose file. This is the way.PrerequisitesBefore getting started, ensure that you have the following tools installed on your system:DockerDocker ComposeStep 1: Set Up the Python API AppFirst, let’s create a Python API app using Flask. Create a new directory for your project and navigate to it. Then, create a file named app.py with the following code:from flask import Flask, jsonify, requestfrom prometheus_client import make_wsgi_app, Counter, Histogramfrom werkzeug.middleware.dispatcher import DispatcherMiddlewareimport timeapp = Flask(__name__)app.wsgi_app = DispatcherMiddleware(app.wsgi_app, {    \\'/metrics\\': make_wsgi_app()})REQUEST_COUNT = Counter(    \\'app_request_count\\',    \\'Application Request Count\\',    [\\'method\\', \\'endpoint\\', \\'http_status\\'])REQUEST_LATENCY = Histogram(    \\'app_request_latency_seconds\\',    \\'Application Request Latency\\',    [\\'method\\', \\'endpoint\\'])@app.route(\\'/\\')def hello():    start_time = time.time()    REQUEST_COUNT.labels(\\'GET\\', \\'/\\', 200).inc()response = jsonify(message=\\'Hello, world!\\')    REQUEST_LATENCY.labels(\\'GET\\', \\'/\\').observe(time.time() - start_time)    return responseif __name__ == \\'__main__\\':    app.run(host=\\'0.0.0.0\\', port=5000)Save the file.Step 2: Create a DockerfileNext, let’s create a Dockerfile to package our Python app. Create a file named Dockerfile with the following content:FROM python:3.9-slimWORKDIR /appCOPY requirements.txt .RUN pip install --no-cache-dir -r requirements.txtCOPY app.py .CMD [ \"python\", \"app.py\" ]This Dockerfile sets up a Python 3.9 environment, installs the required dependencies, and copies the app.py file into the container. It then runs the Python app when the container starts.Also, do not forget the requirements.txt below:flaskprometheus-clientStep 3: Create a Docker Compose FileNow, let’s create a Docker Compose file to run our app and Prometheus together. Create a file named docker-compose.yml with the following content:version: \\'3\\'services:  app:    build:      context: .      dockerfile: Dockerfile    ports:      - 5000:5000    environment:      - PYTHONUNBUFFERED=1    depends_on:      - prometheus  prometheus:    image: prom/prometheus    ports:      - 9090:9090    volumes:      - ./prometheus.yml:/etc/prometheus/prometheus.ymlThis Docker Compose file defines two services: app for our Python API app, and prometheus for Prometheus. We also mount a prometheus.yml configuration file to the Prometheus container.Step 4: Configure PrometheusCreate a file named prometheus.yml in the same directory as the docker-compose.yml file, and add the following content:global:  scrape_interval: 10sscrape_configs:  - job_name: \\'app\\'    metrics_path: \\'/metrics\\'    static_configs:      - targets: [\\'app:5000\\']This configuration tells Prometheus to scrape metrics from our app service at http://app:5000/metrics every 10 seconds.Step 5: Build and Run the App with PrometheusNow, we’re ready to build and run our app with Prometheus. Open your terminal, navigate to the project directory, and run the following command:docker-compose up --buildThis command will build the Docker images and start the containers defined in the docker-compose.yml file.Once the containers are running, you can access the API at http://localhost:5000 and the Prometheus dashboard at http://localhost:9090. The /metrics endpoint of the API will expose the metrics that Prometheus can scrape.Step 6: Let’s try it!First, verify if Prometheus is running and the app is UP, like the image below:make sure that the state is green “up”Now go to the Graph tab and try to find the metrics we created ( any app related metrics), Prometheus has auto-complete, so its hela easy to find it if it exists:The metrics are being captured by Prometheus!Awesome! Now feel free to play with the graphics and the metrics we created:ConclusionWell done. You have successfully exposed metrics for a simple Python API app into Prometheus. We instrumented the app to track request counts and latencies. Using Docker Compose, you can easily run the app and Prometheus together for monitoring and analysis.This is a brief demonstration on how useful it is to instrument an application with Prometheus to have observability out of the box.You can do even more exposing erros for example, so you can see with metrics all the stuff you are used to see with logging.Pro tip: Metrics is way cheaper.See you in the next one.Peace.ObservabilityPythonDevOpsDockerO11y----FollowWritten by Adso95 Followers·36 FollowingPlaftorm Engineering - ObservabilityFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader,BSHTMLLoader\n",
    "\n",
    "path = \"C://Users//jarvi//OneDrive//Desktop//tehwei//github//LLMOps-Project//research//rag//Exposing Python Metrics with Prometheus _ by Adso _ Medium _ Medium.html\"\n",
    "\n",
    "## Speed and efficiency\n",
    "loader = BSHTMLLoader(Path(path),open_encoding=\"utf-8\")\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Exposing Python Metrics with Prometheus by Adso Medium MediumOpen in appSign upSign inWriteSign upSign inExposing Python Metrics with PrometheusAdsoFollow4 min readJun 26, 2023--ListenSharePrometheus is a powerful monitoring and alerting system that collects and stores time-series data. In this step-by-step guide, we will demonstrate how to expose metrics for a simple Python API app and monitor them using Prometheus. We will also instrument the app to track request metrics. To make it easier for you to try it out, we will provide all the necessary code blocks and a Docker Compose file. This is the way.PrerequisitesBefore getting started, ensure that you have the following tools installed on your system:DockerDocker ComposeStep 1: Set Up the Python API AppFirst, lets create a Python API app using Flask. Create a new directory for your project and navigate to it. Then, create a file named app.py with the following code:from flask import Flask, jsonify, requestfrom prometheus_client import make_wsgi_app, Counter, Histogramfrom werkzeug.middleware.dispatcher import DispatcherMiddlewareimport timeapp Flask(__name__)app.wsgi_app DispatcherMiddleware(app.wsgi_app, metrics: make_wsgi_app())REQUEST_COUNT Counter( app_request_count, Application Request Count, method, endpoint, http_status)REQUEST_LATENCY Histogram( app_request_latency_seconds, Application Request Latency, method, endpoint)app.route()def hello(): start_time time.time() REQUEST_COUNT.labels(GET, , 200).inc()response jsonify(messageHello, world!) REQUEST_LATENCY.labels(GET, ).observe(time.time() - start_time) return responseif __name__ __main__: app.run(host0.0.0.0, port5000)Save the file.Step 2: Create a DockerfileNext, lets create a Dockerfile to package our Python app. Create a file named Dockerfile with the following content:FROM python:3.9-slimWORKDIR appCOPY requirements.txt .RUN pip install --no-cache-dir -r requirements.txtCOPY app.py .CMD python, app.py This Dockerfile sets up a Python 3.9 environment, installs the required dependencies, and copies the app.py file into the container. It then runs the Python app when the container starts.Also, do not forget the requirements.txt below:flaskprometheus-clientStep 3: Create a Docker Compose FileNow, lets create a Docker Compose file to run our app and Prometheus together. Create a file named docker-compose.yml with the following content:version: 3services: app: build: context: . dockerfile: Dockerfile ports: - 5000:5000 environment: - PYTHONUNBUFFERED1 depends_on: - prometheus prometheus: image: promprometheus ports: - 9090:9090 volumes: - .prometheus.yml:etcprometheusprometheus.ymlThis Docker Compose file defines two services: app for our Python API app, and prometheus for Prometheus. We also mount a prometheus.yml configuration file to the Prometheus container.Step 4: Configure PrometheusCreate a file named prometheus.yml in the same directory as the docker-compose.yml file, and add the following content:global: scrape_interval: 10sscrape_configs: - job_name: app metrics_path: metrics static_configs: - targets: app:5000This configuration tells Prometheus to scrape metrics from our app service at http:app:5000metrics every 10 seconds.Step 5: Build and Run the App with PrometheusNow, were ready to build and run our app with Prometheus. Open your terminal, navigate to the project directory, and run the following command:docker-compose up --buildThis command will build the Docker images and start the containers defined in the docker-compose.yml file.Once the containers are running, you can access the API at http:localhost:5000 and the Prometheus dashboard at http:localhost:9090. The metrics endpoint of the API will expose the metrics that Prometheus can scrape.Step 6: Lets try it!First, verify if Prometheus is running and the app is UP, like the image below:make sure that the state is green upNow go to the Graph tab and try to find the metrics we created ( any app related metrics), Prometheus has auto-complete, so its hela easy to find it if it exists:The metrics are being captured by Prometheus!Awesome! Now feel free to play with the graphics and the metrics we created:ConclusionWell done. You have successfully exposed metrics for a simple Python API app into Prometheus. We instrumented the app to track request counts and latencies. Using Docker Compose, you can easily run the app and Prometheus together for monitoring and analysis.This is a brief demonstration on how useful it is to instrument an application with Prometheus to have observability out of the box.You can do even more exposing erros for example, so you can see with metrics all the stuff you are used to see with logging.Pro tip: Metrics is way cheaper.See you in the next one.Peace.ObservabilityPythonDevOpsDockerO11y----FollowWritten by Adso95 Followers36 FollowingPlaftorm Engineering - ObservabilityFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text = clean_text(doc[0].page_content)\n",
    "print(len(full_text))\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Open in app Sign in Write Sign in Exposing Python Metrics with Prometheus Adso Follow 4 min read Jun 26, 2023 -- Prometheus is a powerful monitoring and alerting system that collects and stores time-series data. In this step-by-step guide, we will demonstrate how to expose metrics for a simple Python API app and monitor them using Prometheus. We will also instrument the app to track request metrics. To make it easier for you to try it out, we will provide all the necessary code blocks and a Docker Compose file. This is the way. Prerequisites Before getting started, ensure that you have the following tools installed on your system: Docker Docker Compose Step 1: Set Up the Python API App First, lets create a Python API app using Flask. Create a new directory for your project and navigate to it. Then, create a file named app.py with the following code: from flask import Flask, jsonify, request from prometheus_client import make_wsgi_app, Counter, Histogram from werkzeug.middleware.dispatcher import DispatcherMiddleware import timeapp Flask(__name__)app.wsgi_app DispatcherMiddleware(app.wsgi_app, metrics: make_wsgi_app() )REQUEST_COUNT Counter( app_request_count, Application Request Count, method, endpoint, http_status )REQUEST_LATENCY Histogram( app_request_latency_seconds, Application Request Latency, method, endpoint )app.route() def hello(): start_time time.time() REQUEST_COUNT.labels(GET, , 200).inc()response jsonify(messageHello, world!) REQUEST_LATENCY.labels(GET, ).observe(time.time() - start_time) return responseif __name__ __main__: app.run(host0.0.0.0, port5000) Save the file. Step 2: Create a Dockerfile Next, lets create a Dockerfile to package our Python app. Create a file named Dockerfile with the following content: FROM python:3.9-slim WORKDIR app COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt COPY app.py . CMD python, app.py This Dockerfile sets up a Python 3.9 environment, installs the required dependencies, and copies the app.py file into the container. It then runs the Python app when the container starts. Also, do not forget the requirements.txt below: flask prometheus-client Step 3: Create a Docker Compose File Now, lets create a Docker Compose file to run our app and Prometheus together. Create a file named docker-compose.yml with the following content: version: 3 services: app: build: context: . dockerfile: Dockerfile ports: - 5000:5000 environment: - PYTHONUNBUFFERED1 depends_on: - prometheus prometheus: image: promprometheus ports: - 9090:9090 volumes: - .prometheus.yml:etcprometheusprometheus.yml This Docker Compose file defines two services: app for our Python API app, and prometheus for Prometheus. We also mount a prometheus.yml configuration file to the Prometheus container. Step 4: Configure Prometheus Create a file named prometheus.yml in the same directory as the docker-compose.yml file, and add the following content: global: scrape_interval: 10s scrape_configs: - job_name: app metrics_path: metrics static_configs: - targets: app:5000 This configuration tells Prometheus to scrape metrics from our app service at http:app:5000metrics every 10 seconds. Step 5: Build and Run the App with Prometheus Now, were ready to build and run our app with Prometheus. Open your terminal, navigate to the project directory, and run the following command: docker-compose up --build This command will build the Docker images and start the containers defined in the docker-compose.yml file. Once the containers are running, you can access the API at http:localhost:5000 and the Prometheus dashboard at http:localhost:9090. The metrics endpoint of the API will expose the metrics that Prometheus can scrape. Step 6: Lets try it! First, verify if Prometheus is running and the app is UP, like the image below: Now go to the Graph tab and try to find the metrics we created ( any app related metrics), Prometheus has auto-complete, so its hela easy to find it if it exists: Awesome! Now feel free to play with the graphics and the metrics we created: Conclusion Well done. You have successfully exposed metrics for a simple Python API app into Prometheus. We instrumented the app to track request counts and latencies. Using Docker Compose, you can easily run the app and Prometheus together for monitoring and analysis. This is a brief demonstration on how useful it is to instrument an application with Prometheus to have observability out of the box. You can do even more exposing erros for example, so you can see with metrics all the stuff you are used to see with logging. Pro tip: Metrics is way cheaper. See you in the next one. Peace. Observability Python DevOps Docker O11y -- -- Written by Adso 95 Followers 36 Following Plaftorm Engineering - Observability No responses yet Help Status About Careers Press Blog Privacy Terms Text to speech Teams'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Handle more complex elements, but slower processing time\n",
    "loader = UnstructuredHTMLLoader(Path(path))\n",
    "doc = loader.load()\n",
    "full_text = clean_text(doc[0].page_content)\n",
    "print(len(full_text))\n",
    "full_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
